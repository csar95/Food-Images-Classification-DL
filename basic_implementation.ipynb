{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0Mr2JF7Lhbclw2RnRfIix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csar95/Food-Images-Classification-DL/blob/main/basic_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de datos\n",
        "Vamos a usar un dataset de imágenes de comida disponible en [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). "
      ],
      "metadata": {
        "id": "QyuLa8K-qBsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq2LBdqOpL92",
        "outputId": "729528e0-e3c6-4aa6-ba3e-ec641857fc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading food11-image-dataset.zip to /content\n",
            "100% 1.08G/1.08G [00:29<00:00, 43.4MB/s]\n",
            "100% 1.08G/1.08G [00:29<00:00, 40.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"cesargutic\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"\"\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se habrán descargado 3 carpetas diferentes para los datos de entrenamiento, validación y evaluación, dentro de las cuales se encuentra una subcarpeta para cada una de las 11 clases de comida:\n",
        "\n",
        "- Bread (panes)\n",
        "- Dairy product (lácteos)\n",
        "- Dessert (postres)\n",
        "- Egg (huevos)\n",
        "- Fried food (fritos)\n",
        "- Meat (carnes)\n",
        "- Noodles-Pasta (pasta)\n",
        "- Rice (arroz)\n",
        "- Seafood (pescado y marisco)\n",
        "- Soup (sopas)\n",
        "- Vegetable-Fruit (vegetales y frutas)"
      ],
      "metadata": {
        "id": "bu0p3B0HqTa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINDIR = \"/content/training\"\n",
        "VALDIR = \"/content/validation\"\n",
        "TESTDIR = \"/content/evaluation\""
      ],
      "metadata": {
        "id": "zG6UBmpypzZS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducción de clases\n",
        "Con el fin de hacer este problema más accesible, vamos a centrarnos solo en seis de las clases de comida disponibles: Bread, Dairy product, Dessert, Egg, Fried food y Meat."
      ],
      "metadata": {
        "id": "ZWNLET7Lqswe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ZruxXQp5L2",
        "outputId": "a427ab16-faae-432c-e32c-c8668d0e9949"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting /content/evaluation/Vegetable-Fruit...\n",
            "Deleting /content/evaluation/Seafood...\n",
            "Deleting /content/evaluation/Noodles-Pasta...\n",
            "Deleting /content/evaluation/Rice...\n",
            "Deleting /content/evaluation/Soup...\n",
            "Deleting /content/validation/Vegetable-Fruit...\n",
            "Deleting /content/validation/Seafood...\n",
            "Deleting /content/validation/Noodles-Pasta...\n",
            "Deleting /content/validation/Rice...\n",
            "Deleting /content/validation/Soup...\n",
            "Deleting /content/training/Vegetable-Fruit...\n",
            "Deleting /content/training/Seafood...\n",
            "Deleting /content/training/Noodles-Pasta...\n",
            "Deleting /content/training/Rice...\n",
            "Deleting /content/training/Soup...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesando imágenes desde ficheros\n",
        "Este dataset de imágenes es grande, con imágenes de buena resolución, y cada una de ellas tiene diferentes tamaños y relación de aspecto.\n",
        "\n",
        "A continuación realizamos el trabajo de carga y procesamiento de las imágenes."
      ],
      "metadata": {
        "id": "oHrnOUvDq9T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "Pv43eRZosdIb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "transformations = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "}\n",
        "\n",
        "imageFolders = {\n",
        "    _set: datasets.ImageFolder(eval(f\"{_set.upper()}DIR\"), transform=transformations[_set])\n",
        "    for _set in [\"train\", \"val\", \"test\"]\n",
        "}\n",
        "\n",
        "dataLoaders = {\n",
        "    _set: DataLoader(imageFolders[_set], batch_size=batch_size, shuffle=True)\n",
        "    for _set in [\"train\", \"val\", \"test\"]\n",
        "}"
      ],
      "metadata": {
        "id": "7gqrDDQr6rzF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageFolders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqcOFHAY8u79",
        "outputId": "49665a21-00ac-4f9e-b6c2-8d7e44e15553"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset ImageFolder\n",
              "     Number of datapoints: 6082\n",
              "     Root location: /content/training\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
              "                ToTensor()\n",
              "            ), 'val': Dataset ImageFolder\n",
              "     Number of datapoints: 2108\n",
              "     Root location: /content/validation\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
              "                ToTensor()\n",
              "            ), 'test': Dataset ImageFolder\n",
              "     Number of datapoints: 2070\n",
              "     Root location: /content/evaluation\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
              "                ToTensor()\n",
              "            )}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construcción de la red"
      ],
      "metadata": {
        "id": "x-0Agq_mWnDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TOi-DEUBwroK",
        "outputId": "f1668408-814b-418f-d3e3-938d55d79c09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "yQC2Vo_7XRT0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepLearningNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(DeepLearningNet, self).__init__()\n",
        "        # Input size: (BatchSize, 32, 32, 3)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3)),   # (BatchSize, 30, 30, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2,2))                                 # (BatchSize, 15, 15, 32)\n",
        "        )\n",
        "        self.flatten = nn.Flatten(start_dim=1)  # dim=0 es el batch. dim=1 para aplanar cada imagen del batch\n",
        "        self.classifier = nn.Sequential(                                    # (BatchSize, 7200)\n",
        "            nn.Linear(in_features=7200, out_features=128),                  # (BatchSize, 128)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(in_features=128, out_features=6),                     # (BatchSize, 6)\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input):\n",
        "        output = self.features(input)        \n",
        "        output = self.flatten(output)        \n",
        "        output = self.classifier(output)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "p02p7R1OvsT4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepLearningNet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2uzsXj0a6rF",
        "outputId": "82dc3ebe-645d-45d8-fad2-affbe64e79e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepLearningNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=7200, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=6, bias=True)\n",
              "    (4): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "UBKK4eBPbyr_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "oZdkJXfYcEiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = next(iter(dataLoaders['train']))\n",
        "img.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CauiSIcydEXs",
        "outputId": "18488529-9eca-43c3-becb-5c7bf8d3ebe4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3, 32, 32]),\n",
              " tensor([2, 5, 5, 2, 4, 3, 0, 1, 3, 0, 5, 2, 1, 3, 5, 3, 2, 4, 0, 0, 3, 3, 2, 2,\n",
              "         4, 0, 2, 0, 5, 5, 5, 2, 4, 0, 2, 2, 5, 0, 0, 5, 2, 3, 5, 5, 0, 5, 2, 5,\n",
              "         2, 2, 3, 1, 0, 1, 4, 4, 3, 3, 2, 3, 4, 4, 2, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(img)\n",
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-atCOQEgdJEP",
        "outputId": "0921cab5-dc79-4d3d-d766-ea7070775565"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1931, 0.1273, 0.1729, 0.1893, 0.1351, 0.1822],\n",
              "        [0.1847, 0.1519, 0.1728, 0.1617, 0.1418, 0.1872],\n",
              "        [0.1817, 0.1309, 0.1683, 0.1753, 0.1675, 0.1763],\n",
              "        [0.1931, 0.1446, 0.1635, 0.1519, 0.1422, 0.2048],\n",
              "        [0.1847, 0.1486, 0.1723, 0.1645, 0.1532, 0.1768],\n",
              "        [0.1891, 0.1425, 0.1754, 0.1712, 0.1422, 0.1796],\n",
              "        [0.1925, 0.1364, 0.1511, 0.1785, 0.1566, 0.1850],\n",
              "        [0.1805, 0.1406, 0.1775, 0.1744, 0.1432, 0.1837],\n",
              "        [0.1793, 0.1438, 0.1778, 0.1646, 0.1592, 0.1754],\n",
              "        [0.1825, 0.1380, 0.1683, 0.1709, 0.1508, 0.1895],\n",
              "        [0.1962, 0.1365, 0.1682, 0.1834, 0.1433, 0.1724],\n",
              "        [0.1845, 0.1543, 0.1677, 0.1675, 0.1437, 0.1823],\n",
              "        [0.1808, 0.1349, 0.1738, 0.1796, 0.1556, 0.1753],\n",
              "        [0.1720, 0.1374, 0.1751, 0.1877, 0.1543, 0.1736],\n",
              "        [0.1775, 0.1494, 0.1699, 0.1662, 0.1523, 0.1847],\n",
              "        [0.1680, 0.1353, 0.1868, 0.1751, 0.1642, 0.1707],\n",
              "        [0.1800, 0.1573, 0.1636, 0.1701, 0.1502, 0.1788],\n",
              "        [0.1797, 0.1457, 0.1697, 0.1542, 0.1574, 0.1934],\n",
              "        [0.1911, 0.1557, 0.1466, 0.1732, 0.1526, 0.1808],\n",
              "        [0.1940, 0.1424, 0.1770, 0.1777, 0.1330, 0.1759],\n",
              "        [0.1700, 0.1289, 0.1845, 0.1723, 0.1589, 0.1854],\n",
              "        [0.1918, 0.1491, 0.1673, 0.1621, 0.1560, 0.1737],\n",
              "        [0.1780, 0.1468, 0.1711, 0.1818, 0.1516, 0.1708],\n",
              "        [0.1788, 0.1547, 0.1810, 0.1754, 0.1482, 0.1619],\n",
              "        [0.1794, 0.1333, 0.1763, 0.1687, 0.1555, 0.1868],\n",
              "        [0.1892, 0.1457, 0.1703, 0.1702, 0.1503, 0.1743],\n",
              "        [0.1924, 0.1439, 0.1602, 0.1628, 0.1582, 0.1825],\n",
              "        [0.1853, 0.1547, 0.1658, 0.1585, 0.1555, 0.1802],\n",
              "        [0.1805, 0.1407, 0.1750, 0.1670, 0.1588, 0.1780],\n",
              "        [0.1775, 0.1453, 0.1729, 0.1555, 0.1538, 0.1950],\n",
              "        [0.1928, 0.1317, 0.1705, 0.1746, 0.1425, 0.1879],\n",
              "        [0.1970, 0.1230, 0.1746, 0.1684, 0.1503, 0.1867],\n",
              "        [0.1834, 0.1359, 0.1633, 0.1892, 0.1655, 0.1627],\n",
              "        [0.1854, 0.1524, 0.1645, 0.1685, 0.1552, 0.1741],\n",
              "        [0.1880, 0.1426, 0.1669, 0.1711, 0.1514, 0.1800],\n",
              "        [0.1712, 0.1458, 0.1786, 0.1618, 0.1722, 0.1704],\n",
              "        [0.1810, 0.1297, 0.1799, 0.1785, 0.1551, 0.1758],\n",
              "        [0.1656, 0.1426, 0.1794, 0.1727, 0.1485, 0.1911],\n",
              "        [0.1893, 0.1416, 0.1471, 0.1905, 0.1563, 0.1751],\n",
              "        [0.1665, 0.1491, 0.1837, 0.1685, 0.1439, 0.1883],\n",
              "        [0.1772, 0.1356, 0.1632, 0.1754, 0.1589, 0.1897],\n",
              "        [0.1796, 0.1537, 0.1583, 0.1689, 0.1459, 0.1936],\n",
              "        [0.1776, 0.1416, 0.1694, 0.1831, 0.1503, 0.1779],\n",
              "        [0.1769, 0.1389, 0.1893, 0.1650, 0.1481, 0.1818],\n",
              "        [0.1756, 0.1360, 0.1685, 0.1839, 0.1632, 0.1728],\n",
              "        [0.1824, 0.1480, 0.1626, 0.1820, 0.1546, 0.1705],\n",
              "        [0.2083, 0.1434, 0.1525, 0.1876, 0.1184, 0.1899],\n",
              "        [0.1874, 0.1329, 0.1832, 0.1789, 0.1483, 0.1694],\n",
              "        [0.1805, 0.1462, 0.1680, 0.1663, 0.1641, 0.1749],\n",
              "        [0.1755, 0.1393, 0.1896, 0.1627, 0.1543, 0.1787],\n",
              "        [0.1855, 0.1373, 0.1610, 0.1620, 0.1674, 0.1869],\n",
              "        [0.1830, 0.1492, 0.1667, 0.1749, 0.1525, 0.1736],\n",
              "        [0.2008, 0.1465, 0.1467, 0.1837, 0.1496, 0.1726],\n",
              "        [0.1837, 0.1413, 0.1677, 0.1805, 0.1526, 0.1742],\n",
              "        [0.1889, 0.1217, 0.1675, 0.1956, 0.1548, 0.1716],\n",
              "        [0.1659, 0.1435, 0.1696, 0.1742, 0.1717, 0.1752],\n",
              "        [0.1828, 0.1439, 0.1613, 0.1865, 0.1489, 0.1766],\n",
              "        [0.1829, 0.1417, 0.1697, 0.1695, 0.1509, 0.1853],\n",
              "        [0.1666, 0.1414, 0.1629, 0.1636, 0.1706, 0.1949],\n",
              "        [0.1976, 0.1281, 0.1686, 0.1836, 0.1225, 0.1995],\n",
              "        [0.1891, 0.1401, 0.1731, 0.1657, 0.1436, 0.1883],\n",
              "        [0.1936, 0.1457, 0.1550, 0.1724, 0.1577, 0.1756],\n",
              "        [0.1890, 0.1387, 0.1596, 0.1795, 0.1516, 0.1816],\n",
              "        [0.1809, 0.1291, 0.1829, 0.1706, 0.1540, 0.1825]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "num_epochs = 20\n",
        "patience = 7  # Number of epochs with no improvement after which training will be stopped\n",
        "early_stopping = False\n",
        "best_loss = float('inf')\n",
        "epochs_with_no_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_results = {}\n",
        "\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        # This sets the execution mode and informs layers (e.g., Dropout, BatchNorm) designed to behave differently during training and evaluation\n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        correct_in_dataset = 0\n",
        "        \n",
        "        # For each batch of images update model parameters / weights\n",
        "        for i, (inputs, labels) in enumerate(dataLoaders[phase]):        \n",
        "            optimizer.zero_grad()               # Sets the gradients of all optimized tensors to zero. Same as model.zero_grad() if all model parameters are in the optimizer\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if phase == \"train\":\n",
        "                loss.backward()                 # Computes the gradient of loss w.r.t all the parameters in loss that have requires_grad=True and store them in x.grad (x.grad += dloss/dx)\n",
        "                optimizer.step()                # Performs a single optimization step (parameter update based on the gradients)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_in_dataset += (predicted == labels).sum().item()\n",
        "        \n",
        "        if phase == \"train\":\n",
        "            epoch_results['train_loss'] = running_loss/len(dataLoaders[phase])\n",
        "            epoch_results['train_accuracy'] = correct_in_dataset/len(imageFolders[phase])\n",
        "\n",
        "        if phase == \"val\":\n",
        "            epoch_results['val_loss'] = running_loss/len(dataLoaders[phase])\n",
        "            epoch_results['val_accuracy'] = correct_in_dataset/len(imageFolders[phase])\n",
        "            \n",
        "            if epoch_results['val_loss'] < best_loss:\n",
        "                best_loss = epoch_results['val_loss']\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "                epochs_with_no_improvement = 0\n",
        "            else:\n",
        "                epochs_with_no_improvement += 1\n",
        "\n",
        "            if epochs_with_no_improvement == patience:\n",
        "                model.load_state_dict(best_model)\n",
        "                early_stopping = True\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | loss: {epoch_results['train_loss']:.4} - accuracy: {epoch_results['train_accuracy']:.4} - val_loss: {epoch_results['val_loss']:.4} - val_accuracy: {epoch_results['val_accuracy']:.4}\")\n",
        "\n",
        "    if early_stopping:\n",
        "        print('Early stopping!')\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689b6df6-ae5c-4c3c-f2d9-8b27598b1c3a",
        "id": "UNnRT14wAY9G"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | loss: 1.722 - accuracy: 0.2964 - val_loss: 1.68 - val_accuracy: 0.3344\n",
            "Epoch 2/20 | loss: 1.669 - accuracy: 0.3589 - val_loss: 1.634 - val_accuracy: 0.3952\n",
            "Epoch 3/20 | loss: 1.638 - accuracy: 0.3889 - val_loss: 1.627 - val_accuracy: 0.3961\n",
            "Epoch 4/20 | loss: 1.626 - accuracy: 0.4025 - val_loss: 1.638 - val_accuracy: 0.3866\n",
            "Epoch 5/20 | loss: 1.617 - accuracy: 0.4155 - val_loss: 1.616 - val_accuracy: 0.4122\n",
            "Epoch 6/20 | loss: 1.608 - accuracy: 0.4308 - val_loss: 1.617 - val_accuracy: 0.4099\n",
            "Epoch 7/20 | loss: 1.594 - accuracy: 0.442 - val_loss: 1.603 - val_accuracy: 0.4303\n",
            "Epoch 8/20 | loss: 1.577 - accuracy: 0.4558 - val_loss: 1.601 - val_accuracy: 0.4213\n",
            "Epoch 9/20 | loss: 1.572 - accuracy: 0.4661 - val_loss: 1.599 - val_accuracy: 0.425\n",
            "Epoch 10/20 | loss: 1.56 - accuracy: 0.4873 - val_loss: 1.6 - val_accuracy: 0.4265\n",
            "Epoch 11/20 | loss: 1.55 - accuracy: 0.4961 - val_loss: 1.587 - val_accuracy: 0.4478\n",
            "Epoch 12/20 | loss: 1.54 - accuracy: 0.5013 - val_loss: 1.588 - val_accuracy: 0.4454\n",
            "Epoch 13/20 | loss: 1.526 - accuracy: 0.5158 - val_loss: 1.589 - val_accuracy: 0.4402\n",
            "Epoch 14/20 | loss: 1.524 - accuracy: 0.5291 - val_loss: 1.599 - val_accuracy: 0.4298\n",
            "Epoch 15/20 | loss: 1.513 - accuracy: 0.5421 - val_loss: 1.584 - val_accuracy: 0.445\n",
            "Epoch 16/20 | loss: 1.5 - accuracy: 0.5455 - val_loss: 1.6 - val_accuracy: 0.4236\n",
            "Epoch 17/20 | loss: 1.488 - accuracy: 0.5556 - val_loss: 1.588 - val_accuracy: 0.4383\n",
            "Epoch 18/20 | loss: 1.486 - accuracy: 0.563 - val_loss: 1.582 - val_accuracy: 0.4459\n",
            "Epoch 19/20 | loss: 1.474 - accuracy: 0.5807 - val_loss: 1.584 - val_accuracy: 0.4412\n",
            "Epoch 20/20 | loss: 1.47 - accuracy: 0.5862 - val_loss: 1.589 - val_accuracy: 0.445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación"
      ],
      "metadata": {
        "id": "Gn1LIyXwcrKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataLoaders['test']:        \n",
        "        outputs = model(images)        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "print(f\"Accuracy: {correct / len(imageFolders['test']):.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLLK2A-V2ZUc",
        "outputId": "ab96f177-25e0-42b0-9744-171f270c6915"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 45.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9I8wPZJF2adD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
